{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModelWithProjection\n",
    "\n",
    "clip_model = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    pretrained_model_name_or_path='openai/clip-vit-large-patch14',\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "clip_model.to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Dinov2Model\n",
    "\n",
    "dv2_model = Dinov2Model.from_pretrained(\n",
    "    pretrained_model_name_or_path='facebook/dinov2-base',\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "dv2_model.to(device='cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import src.data.transforms as dT\n",
    "from src import utils\n",
    "config = utils.get_config()\n",
    "wandb_config = utils.load_config('clip.yml')\n",
    "processor = dT.make_training_processor(config, wandb_config)\n",
    "image = Image.open('/home/external-rosia/RosIA/reflection-connection/data/raw/train/Boring/abwao.png')\n",
    "image = image.convert(\"RGB\")\n",
    "for _ in range(10):\n",
    "    inputs = processor.preprocess_image([image])\n",
    "    print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = clip_model(pixel_values=image.to(device='cuda:0'))\n",
    "for k, v in outputs.items():\n",
    "    display(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = dv2_model(pixel_values=image.to(device='cuda:1'))\n",
    "for k, v in outputs.items():\n",
    "    display(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from src.models.losses import make_triplet_criterion\n",
    "\n",
    "input1 = torch.randn(1, 128)\n",
    "anchors = torch.repeat_interleave(input1, repeats=2, dim=0)\n",
    "# input2 = torch.randn(1, 128)\n",
    "positives = torch.repeat_interleave(input1, repeats=2, dim=0)\n",
    "negatives = torch.repeat_interleave(-input1, repeats=2, dim=0)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "display(1 - cos(anchors, positives))\n",
    "display(1 - cos(anchors, negatives))\n",
    "criterion = make_triplet_criterion({'criterion': 'TMWDL-Cosine'})\n",
    "criterion(anchors, negatives, positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.submissions import search\n",
    "from PIL import Image\n",
    "from src import utils\n",
    "import torch\n",
    "\n",
    "config = utils.get_config()\n",
    "wandb_run = utils.get_run('2khs9u4f')\n",
    "model = search.RefConInferenceModel(config, wandb_run, 0)\n",
    "image = Image.open('/home/external-rosia/RosIA/reflection-connection/data/raw/train/Boring/abwao.png')\n",
    "image = image.convert(\"RGB\")\n",
    "outputs = model(images=image)\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.submissions import search\n",
    "from PIL import Image\n",
    "from src import utils\n",
    "import torch\n",
    "\n",
    "config = utils.get_config()\n",
    "wandb_run = utils.get_run('2khs9u4f')\n",
    "\n",
    "query_set = search.ImageSet(config, wandb_run, query=True, cuda_idx=0)\n",
    "corpus_set = search.ImageSet(config, wandb_run, query=False, cuda_idx=0)\n",
    "query_set.build_embeddings()\n",
    "corpus_set.build_embeddings()\n",
    "sbf = search.SearchBruteForce(corpus_set)\n",
    "sbf.query(query_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "import plotly.express as px\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "# model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "px.imshow(image).show()\n",
    "inputs = processor(images=image, return_tensors=\"pt\", do_normalize=False)\n",
    "process_image = inputs['pixel_values'].squeeze().movedim(0, 2).numpy(force=True) * 255\n",
    "\n",
    "px.imshow(process_image.tolist()).show()\n",
    "# outputs = model(**inputs)\n",
    "# last_hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "import torch\n",
    "import torchvision.transforms.functional as tvF\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "torch.tensor()\n",
    "list_image = [Image.open(path)for path in ['/home/external-rosia/RosIA/reflection-connection/data/raw/train/Boring/ztqsz.png']*2]\n",
    "tvF.to_tensor(list_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import typing\n",
    "arr = np.concatenate([np.array([]), ['a', 'b']])\n",
    "arr[[1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np \n",
    "dataSetI = [[.1, .2, .3]]\n",
    "dataSetI_I = [.4, .5, .6]\n",
    "dataSetII = [.4, .5, .6]\n",
    "# dataSetII = [.1, .2, .3]\n",
    "\n",
    "x = np.array(dataSetI*3).astype(np.float32)\n",
    "print(x)\n",
    "q = np.array([dataSetII]).astype(np.float32)\n",
    "print(q)\n",
    "index = faiss.index_factory(3, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "faiss.normalize_L2(x)\n",
    "index.add(x)\n",
    "x2 = np.array([dataSetI_I]*3).astype(np.float32)\n",
    "faiss.normalize_L2(x2)\n",
    "index2 = faiss.index_factory(3, \"Flat\", faiss.METRIC_INNER_PRODUCT)\n",
    "index2.add(x2)\n",
    "index.merge_from(index2)\n",
    "print(index.ntotal)\n",
    "\n",
    "# index = faiss.index_cpu_to_all_gpus(index)\n",
    "faiss.normalize_L2(q)\n",
    "distance, return_index = index.search(q, 5)\n",
    "print('Similarity by FAISS:{}'.format(distance))\n",
    "print('Index by FAISS:{}'.format(return_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 1.0842e-19, 2.1684e-19, 3.2526e-19],\n",
       "         [4.3368e-19, 5.4210e-19, 6.5052e-19, 7.5894e-19],\n",
       "         [8.6736e-19, 9.7578e-19, 1.0842e-18, 1.1926e-18],\n",
       "         [1.3010e-18, 1.4095e-18, 1.5179e-18, 1.6263e-18]],\n",
       "\n",
       "        [[1.7347e-18, 1.8431e-18, 1.9516e-18, 2.0600e-18],\n",
       "         [2.1684e-18, 2.2768e-18, 2.3852e-18, 2.4937e-18],\n",
       "         [2.6021e-18, 2.7105e-18, 2.8189e-18, 2.9273e-18],\n",
       "         [3.0358e-18, 3.1442e-18, 3.2526e-18, 3.3610e-18]],\n",
       "\n",
       "        [[3.4694e-18, 3.5779e-18, 3.6863e-18, 3.7947e-18],\n",
       "         [3.9031e-18, 4.0115e-18, 4.1200e-18, 4.2284e-18],\n",
       "         [4.3368e-18, 4.4452e-18, 4.5536e-18, 4.6621e-18],\n",
       "         [4.7705e-18, 4.8789e-18, 4.9873e-18, 5.0958e-18]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 1.0842e-19, 2.1684e-19, 3.2526e-19],\n",
       "         [4.3368e-19, 5.4210e-19, 6.5052e-19, 7.5894e-19],\n",
       "         [8.6736e-19, 9.7578e-19, 1.0842e-18, 1.1926e-18],\n",
       "         [1.3010e-18, 1.4095e-18, 1.5179e-18, 1.6263e-18]],\n",
       "\n",
       "        [[1.7347e-18, 1.8431e-18, 1.9516e-18, 2.0600e-18],\n",
       "         [2.1684e-18, 2.2768e-18, 2.3852e-18, 2.4937e-18],\n",
       "         [2.6021e-18, 2.7105e-18, 2.8189e-18, 2.9273e-18],\n",
       "         [3.0358e-18, 3.1442e-18, 3.2526e-18, 3.3610e-18]],\n",
       "\n",
       "        [[3.4694e-18, 3.5779e-18, 3.6863e-18, 3.7947e-18],\n",
       "         [3.9031e-18, 4.0115e-18, 4.1200e-18, 4.2284e-18],\n",
       "         [4.3368e-18, 4.4452e-18, 4.5536e-18, 4.6621e-18],\n",
       "         [4.7705e-18, 4.8789e-18, 4.9873e-18, 5.0958e-18]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2.functional as tvF\n",
    "\n",
    "img = torch.arange(3*4*4).view(3,4,4)\n",
    "img = tvF.to_image(img)\n",
    "img = tvF.to_dtype_image(img, torch.float32, scale=True)\n",
    "display(img)\n",
    "tvF.to_dtype_image(img, torch.float32, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Boring', 'Bright_Planar', 'Fault']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_class_for_image(image_name, class_folders):\n",
    "    for folder in class_folders:\n",
    "        if image_name in os.listdir(folder):\n",
    "            return os.path.basename(folder)\n",
    "    return None\n",
    "\n",
    "def get_classes_for_images(image_names, parent_folder):\n",
    "    class_folders = [os.path.join(parent_folder, folder) for folder in os.listdir(parent_folder) if os.path.isdir(os.path.join(parent_folder, folder))]\n",
    "    classes = [get_class_for_image(image_name, class_folders) for image_name in image_names]\n",
    "    return classes\n",
    "\n",
    "# Exemple d'utilisation\n",
    "image_names = [\"actea.png\", \"akytm.png\", \"abhkh.png\"]  # Liste des noms d'images\n",
    "parent_folder = \"../data/raw/train\"  # Chemin du dossier parent contenant les sous-dossiers/classes\n",
    "\n",
    "classes = get_classes_for_images(image_names, parent_folder)\n",
    "print(classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmenting-subsurface-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
