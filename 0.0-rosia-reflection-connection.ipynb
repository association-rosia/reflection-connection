{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8Ô∏è‚É£ Reflection Connection: Bringing New Algorithms to Old Data\n",
    "\n",
    "<img src=assets/reflection-connection.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåã Context\n",
    "\n",
    "For those unfamiliar with geophysics, seismic images can appear enigmatic: layers of black-and-white waveforms stacked atop one another. However, with increasing familiarity, distinct features within seismic data become discernible. These features signify prevalent geological formations such as river channels, salt pans, or faults. The process of identifying seismic features parallels that of a medical professional distinguishing between anatomical structures on an echocardiogram. Geoscientists amalgamate these identified features to formulate hypotheses regarding the geological evolution of the surveyed area. An algorithm capable of pinpointing specific segments within seismic images holds the promise of empowering geoscientists to construct more comprehensive hypotheses, thereby facilitating the integration of diverse datasets into a cohesive model of Earth's composition and evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Our approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Libraries\n",
    "\n",
    "Our code run on Python 3.10.13\n",
    "\n",
    "Installing external libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing reflection-connection librarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "import os\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.v2.functional as tvF\n",
    "\n",
    "import wandb\n",
    "\n",
    "from src import utils\n",
    "import src.models.utils as mutils\n",
    "import src.data.make_pretrain_data as mpd\n",
    "import src.data.datasets.inference as inference_d\n",
    "from src.models.inference import EmbeddingsBuilder\n",
    "from src.models.retriever import FaissRetriever\n",
    "from src.submissions.make_submissions import ResultBuilder, dist_to_conf\n",
    "from src.models.iterative import IterativeTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to fake wandb account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B initialisation\n",
    "os.environ[\"WANDB_MODE\"]=\"offline\"\n",
    "wandb_api_key = 'X'*40\n",
    "! wandb login --relogin {wandb_api_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a multiprocessing wrapper because PyTorch struggles to manage between the models loaded on the main process and the subprocesses. It's necessary to pass everything to subprocesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "def multiprocess_wrapper(func):\n",
    "        def wrapper(*args: Any, **kwargs: Any):\n",
    "            p = mp.Process(target=func, args=args, kwargs=kwargs)\n",
    "            p.start()\n",
    "            p.join()\n",
    "            \n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì∏ Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers le r√©pertoire contenant les dossiers de classes\n",
    "base_path = \"data/raw/train/\"\n",
    "# Liste des noms de classe (nom des dossiers)\n",
    "class_names = [folder for folder in os.listdir(base_path) if os.path.isdir(base_path + folder)]\n",
    "# Cr√©ez une grille de sous-graphiques\n",
    "num_classes = len(class_names)\n",
    "num_images_per_class = 5\n",
    "fig, axs = plt.subplots(num_images_per_class, num_classes, figsize=(20, 15))\n",
    "\n",
    "# Parcourez chaque classe\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # Chemin vers le dossier de la classe\n",
    "    class_path = os.path.join(base_path, class_name)\n",
    "\n",
    "    # Liste des fichiers image dans le dossier de la classe\n",
    "    image_files = os.listdir(class_path)\n",
    "\n",
    "    # S√©lectionnez al√©atoirement 10 images de la classe\n",
    "    selected_images = random.sample(image_files, num_images_per_class)\n",
    "\n",
    "    # Affichez les images dans la colonne correspondante\n",
    "    for j, image_file in enumerate(selected_images):\n",
    "        # Chemin complet vers l'image\n",
    "        image_path = os.path.join(class_path, image_file)\n",
    "\n",
    "        # Lisez l'image et affichez-la dans le sous-graphique correspondant\n",
    "        img = Image.open(image_path)\n",
    "        axs[j, i].imshow(img)\n",
    "        axs[j, i].axis('off')\n",
    "\n",
    "    # Ajoutez le nom de la classe comme titre pour la colonne\n",
    "    axs[0, i].set_title(class_name, fontsize=16, fontweight='bold', loc='center', pad=20)\n",
    "\n",
    "# Ajustez l'espacement entre les sous-graphiques\n",
    "plt.tight_layout()\n",
    "\n",
    "# Affichez le graphique de mosa√Øque\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öíÔ∏è Preprocessing\n",
    "\n",
    "In computer vision, the classic preprocessing steps for an image are as follows:\n",
    "\n",
    "Scaling: Allows us to scale the values between 0 and 1. (Using a Min Max Scaler)\n",
    "\n",
    "Normalization: Helps us achieve a Gaussian distribution of values for each channel. (Using a Standard Scaler)\n",
    "\n",
    "Rescaling: If necessary, based on what input the model accepts. (Using a bicubic interpolation)\n",
    "\n",
    "Cropping: If necessary, based on what input the model accepts. (Using random crop for training and center crop for inference)\n",
    "\n",
    "However, we have observed that in the images of our dataset, the objective is to delineate areas of varying brightness between them. That's why we decided to add contrast to highlight these differences in shade between the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_wrapper(func):\n",
    "    def mp_wrapper(image):\n",
    "            manager = mp.Manager()\n",
    "            namespace = manager.Namespace()\n",
    "            p = mp.Process(target=func, args=(image, namespace))\n",
    "            p.start()\n",
    "            p.join()\n",
    "\n",
    "            return namespace.image\n",
    "    \n",
    "    return mp_wrapper\n",
    "        \n",
    "    \n",
    "@processing_wrapper\n",
    "def scale(image, namespace):\n",
    "    image = tvF.to_image(image)\n",
    "    image = tvF.to_dtype_image(image, torch.float32, scale=True)\n",
    "    \n",
    "    namespace.image = np.transpose(image.numpy(force=True), (1, 2, 0))\n",
    "\n",
    "@processing_wrapper\n",
    "def contrast(image, namespace):\n",
    "    image = tvF.to_image(image)\n",
    "    image = tvF.adjust_contrast(image, contrast_factor=10)\n",
    "    \n",
    "    namespace.image = np.transpose(image.numpy(force=True), (1, 2, 0))\n",
    "\n",
    "@processing_wrapper\n",
    "def resize(image, namespace):\n",
    "    image = tvF.to_image(image)\n",
    "    image = tvF.resize(image, size=256, interpolation=tvF.InterpolationMode.BICUBIC)\n",
    "\n",
    "    namespace.image = np.transpose(image.numpy(force=True), (1, 2, 0))\n",
    "\n",
    "@processing_wrapper\n",
    "def crop(image, namespace):\n",
    "    image = tvF.to_image(image)\n",
    "    image = tvF.center_crop(image, output_size=224)\n",
    "\n",
    "    namespace.image = np.transpose(image.numpy(force=True), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the image\n",
    "def plot_image(image, title, subplot_pos):\n",
    "    plt.subplot(*subplot_pos)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(title)\n",
    "    # plt.axis('off')\n",
    "\n",
    "# Create subplots\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "image = Image.open('data/raw/test/image_corpus/afbaz.png').convert(\"RGB\")\n",
    "\n",
    "# Original image\n",
    "plot_image(np.array(image), '0 - Original image', (2, 3, 1))\n",
    "\n",
    "# Scaled image\n",
    "scaled_image = scale(image)\n",
    "plot_image(scaled_image, '1 - Scaled image', (2, 3, 2))\n",
    "\n",
    "# Contrasted image\n",
    "contrasted_image = contrast(scaled_image)\n",
    "plot_image(contrasted_image, '2 - Contrasted image', (2, 3, 3))\n",
    "\n",
    "# Resized image\n",
    "resized_image = resize(contrasted_image)\n",
    "plot_image(resized_image, '3 - Resized image', (2, 3, 4))\n",
    "\n",
    "# Cropped image\n",
    "cropped_image = crop(resized_image)\n",
    "plot_image(cropped_image, '4 - Cropped image', (2, 3, 5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñ®Ô∏è Data Augmentation\n",
    "\n",
    "For data augmentation, we relied on the Patch the Planet: Restore Missing Data challenge datasets. We took all available volumes and created PNG images by cropping the slides according to the distribution of image dimensions in the challenge. These crops were then scaled between 0 and 255 using the image creation function provided in the challenge. This resulted in extracting just over a million images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the image\n",
    "def plot_image(image, title, subplot_pos, fontsize):\n",
    "    plt.subplot(*subplot_pos)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(title, fontsize=fontsize)\n",
    "    # plt.axis('off')\n",
    "\n",
    "volume_name = '0kamixt53o'\n",
    "volume = np.load(f'data/raw/pretrain/patch-the-planet-real-train-data/{volume_name}.npy')\n",
    "slice_idx = random.randint(0, 300)\n",
    "slice_array = volume[slice_idx, :, :].T\n",
    "\n",
    "# Create subplots\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title(f'Volume {volume_name}, slice {slice_idx}', pad=30)\n",
    "plt.axis('off')\n",
    "\n",
    "values, counts = mpd.get_values_counts(utils.get_config())\n",
    "tiles_coords = mpd.get_tiles_coords(values, counts)\n",
    "for i, (x0, x1, y0, y1) in enumerate(tiles_coords):\n",
    "    tile = slice_array[x0:x1, y0:y1]\n",
    "    tile = mpd.normalize_pretrain_slice(tile)\n",
    "    image = Image.fromarray(tile).convert('RGB')\n",
    "    plot_image(image, f'Imange: x_min: {x0}, x_max: {x1}, y_min: {y0}, y_max {y1}', (2, 2, i+1), 8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Pretraining\n",
    "\n",
    "Mettre un texte d'exliation sur le pr√©training dinov2 + vitmae mettre un visuel image originale -> image masqu√©e -> image reconstitu√©e et proposer de lancer un pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Fine-Tuning\n",
    "\n",
    "For model fine-tuning, we employed the triplet loss, removing boring images from the anchors (but not from the pool of negative images) as they were not in the final dataset. We tested two distances: Euclidean and Cosine distances. Several models were tested for their ability to produce one-shot predictions, including [CLIP](https://arxiv.org/pdf/2103.00020.pdf), [DINOv2](https://arxiv.org/pdf/2304.07193.pdf), [VITMAE](https://arxiv.org/pdf/2111.06377v2.pdf). We also tested the [ViT](https://arxiv.org/pdf/2010.11929.pdf) models from PyTorch proposed by Onward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch a fine-tuning using a model by selecting a configuration file\n",
    "@multiprocess_wrapper\n",
    "def fine_tune():\n",
    "    config = utils.get_config()\n",
    "    # CLIP from OpenAI\n",
    "    # wandb_config = utils.init_wandb('fine_tuning/clip.yml')\n",
    "    # DINOv2 from META\n",
    "    # wandb_config = utils.init_wandb('fine_tuning/dinov2.yml')\n",
    "    # VITMAE from META\n",
    "    # wandb_config = utils.init_wandb('fine_tuning/vitmae.yml')\n",
    "    # VIT from Hugging Face\n",
    "    # wandb_config = utils.init_wandb('fine_tuning/vit.yml', 'transformers')\n",
    "    # VIT from Pytorch\n",
    "    wandb_config = utils.init_wandb('fine_tuning/vit.yml', 'torchvision')\n",
    "    trainer = mutils.get_trainer(config)\n",
    "    lightning = mutils.get_lightning(config, wandb_config)\n",
    "    trainer.fit(model=lightning)\n",
    "    wandb.finish()\n",
    "\n",
    "    del lightning, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Model loading issue without using a process\n",
    "fine_tune()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üïµÔ∏è Retriever & Submissions\n",
    "\n",
    "For the similar image search part, we utilized the [FAISS](https://ai.meta.com/tools/faiss/) library provided by Meta. It enables us to conduct a brute force search among all image embeddings in our corpus and retrieve the most similar images efficiently. Additionally, we can choose the distance metric based on the triplet loss. Thus, we implemented two types of retrievers, one based on Euclidean similarity and the other on Cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the ID of the model you wish to use.\n",
    "wandb_id = 'nszfciym'\n",
    "# Specify the Name of the model you wish to use.\n",
    "wandb_name = 'key-lime-pie-110'\n",
    "\n",
    "# If you are using a WandB account to record the runs, use the code below.\n",
    "# wandb_run = utils.get_run(wandb_id)\n",
    "# Otherwise, specify the name and ID of the model and choose the corresponding configuration file for training the model.\n",
    "wandb_run = utils.RunDemo('fine_tuning/vit.yml', id=wandb_id, name=wandb_name, sub_config='torchvision')\n",
    "\n",
    "@multiprocess_wrapper\n",
    "def make_submission(wandb_run):\n",
    "    config = utils.get_config()\n",
    "    # You can adjust the number of workers and batch size based on your system configuration.\n",
    "    embeddings_builder = EmbeddingsBuilder(devices=[0], batch_size=4, num_workers=4)\n",
    "\n",
    "    corpus_dataset = inference_d.make_submission_corpus_inference_dataset(config, wandb_run.config)\n",
    "    corpus_embeddings, corpus_names = embeddings_builder.build_embeddings(config, wandb_run, dataset=corpus_dataset)\n",
    "    query_dataset = inference_d.make_submission_query_inference_dataset(config, wandb_run.config)\n",
    "    query_embeddings, query_names = embeddings_builder.build_embeddings(config, wandb_run, dataset=query_dataset)\n",
    "\n",
    "    metric = utils.get_metric(wandb_run.config)\n",
    "    retriever = FaissRetriever(embeddings_size=corpus_embeddings.shape[1], metric=metric)\n",
    "    retriever.add_to_index(corpus_embeddings, labels=corpus_names)\n",
    "    distances, matched_labels = retriever.query(query_embeddings, k=3)\n",
    "    confidence_scores = dist_to_conf(distances)\n",
    "\n",
    "    # Create submission file\n",
    "    result_builder = ResultBuilder(config['path']['submissions'], k=3)\n",
    "    result_builder(\n",
    "        query_names,\n",
    "        matched_labels,\n",
    "        confidence_scores,\n",
    "        f'{wandb_run.name}-{wandb_run.id}'\n",
    "    )\n",
    "\n",
    "# Model loading issue without using a process\n",
    "make_submission(wandb_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find the result in the `submissions` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÅ Iterative Fine-Tuning\n",
    "\n",
    "The iterative training is based on everything we've presented before; it's heavily inspired by Meta's dataset augmentation process with DINOv2. We start by fine-tuning a model with triplet loss on the challenge data, then we find the most similar images to the training images of each class to augment our dataset. With this new augmented dataset, we rerun fine-tuning on the augmented dataset while keeping the exact same validation dataset. We repeat this process as long as the model shows an improvement in performance.\n",
    "\n",
    "mettre un visuelle du fonctionnement du process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.get_config()\n",
    "iterative_config = utils.load_config('fine_tuning/iterative.yml')\n",
    "curated_folder = os.path.join(config['path']['data'], 'raw', 'train')\n",
    "\n",
    "iterative_trainer = IterativeTrainer(\n",
    "    config,\n",
    "    iterative_config,\n",
    "    curated_folder\n",
    ")\n",
    "\n",
    "# The iterative trainer was built to natively support loading in subprocesses, so it doesn't need a wrapper.\n",
    "iterative_trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßëüèª‚Äçüíª Code Submission\n",
    "\n",
    "If you want to change the configuration of the models, please refer to the YAML file available in the config folder.\n",
    "\n",
    "The script takes images from the data/raw/train folder for training and data/raw/test for inference.\n",
    "\n",
    "In the code submission, we only provide the solution that yielded the best results. However, you can find all our approaches in the preceding cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B initialisation\n",
    "wandb_api_key = 'c2f177f1a9d0a0415a0ec16af4eb4e9ede7bb392'\n",
    "! wandb login --relogin {wandb_api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import wandb\n",
    "\n",
    "from src import utils\n",
    "import src.models.utils as mutils\n",
    "import src.data.datasets.inference as inference_d\n",
    "from src.models.inference import EmbeddingsBuilder\n",
    "from src.models.retriever import FaissRetriever\n",
    "from src.submissions.make_submissions import ResultBuilder, dist_to_conf\n",
    "\n",
    "\n",
    "class RefConPipeline:\n",
    "    def __init__(self, yml_file: str='fine_tuning/vit.yml', sub_config: str='torchvision'):\n",
    "        self.yml_file = yml_file\n",
    "        self.sub_config = sub_config\n",
    "        self.config = utils.get_config()\n",
    "        self.manager = mp.Manager()\n",
    "    \n",
    "    def _train(self, wandb_dict):\n",
    "        wandb_config = utils.init_wandb(self.yml_file, self.sub_config)\n",
    "        wandb_dict['wandb_id'] = wandb.run.id\n",
    "        wandb_dict['wandb_name'] = ''\n",
    "        trainer = mutils.get_trainer(self.config)\n",
    "        lightning = mutils.get_lightning(self.config, wandb_config)\n",
    "        trainer.fit(model=lightning)\n",
    "        wandb.finish()\n",
    "       \n",
    "    def train(self):\n",
    "        wandb_dict = self.manager.dict({'wandb_id': '', 'wandb_name': 'demo-name'})\n",
    "        p = mp.Process(target=self._train, args=(wandb_dict,))\n",
    "        p.start()\n",
    "        p.join()\n",
    "        \n",
    "        return dict(wandb_dict)\n",
    "    \n",
    "    def _predict(self, query_folder: str, corpus_folder: str, wandb_run: utils.RunDemo, k: int, batch_size: int, num_workers: int):\n",
    "        embeddings_builder = EmbeddingsBuilder(devices=wandb_run.config['devices'], batch_size=batch_size, num_workers=num_workers)\n",
    "        corpus_dataset = inference_d.make_submission_inference_dataset(corpus_folder, self.config, wandb_run.config)\n",
    "        corpus_embeddings, corpus_names = embeddings_builder.build_embeddings(self.config, wandb_run, dataset=corpus_dataset)\n",
    "        query_dataset = inference_d.make_submission_inference_dataset(query_folder, self.config, wandb_run.config)\n",
    "        query_embeddings, query_names = embeddings_builder.build_embeddings(self.config, wandb_run, dataset=query_dataset)\n",
    "\n",
    "        metric = utils.get_metric(wandb_run.config)\n",
    "        retriever = FaissRetriever(embeddings_size=corpus_embeddings.shape[1], metric=metric)\n",
    "        retriever.add_to_index(corpus_embeddings, labels=corpus_names)\n",
    "        distances, matched_labels = retriever.query(query_embeddings, k=k)\n",
    "        confidence_scores = dist_to_conf(distances)\n",
    "\n",
    "        # Create submission file\n",
    "        result_builder = ResultBuilder(config['path']['submissions'], k=k)\n",
    "        result_builder(\n",
    "            query_names,\n",
    "            matched_labels,\n",
    "            confidence_scores,\n",
    "            f'{wandb_run.name}-{wandb_run.id}'\n",
    "        )\n",
    "        \n",
    "    def predict(self, query_folder: str, corpus_folder: str, wandb_id: str='nszfciym', wandb_name: str='key-lime-pie-110', k: int=3, batch_size: int=16, num_workers: int=16):\n",
    "        wandb_run = utils.RunDemo(self.yml_file, id=wandb_id, name=wandb_name, sub_config=self.sub_config)\n",
    "        p = mp.Process(target=self._predict, args=(query_folder, corpus_folder, wandb_run, k, batch_size, num_workers))\n",
    "        p.start()\n",
    "        p.join()\n",
    "    \n",
    "    def __call__(self, query_folder, corpus_folder, k: int=3, batch_size: int=16, num_workers: int=16):\n",
    "        wandb_dict = self.train()\n",
    "        self.predict(query_folder=query_folder, corpus_folder=corpus_folder, **wandb_dict, k=k, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÆ Predict pipeline\n",
    "\n",
    "The prediction is done on the images located in the definded folders `corpus_folder` and `query_folder`. The final results are saved in `submissions/key-lime-pie-110-nszfciym.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To predict with our solution\n",
    "ref_conf_pipeline = RefConPipeline()\n",
    "\n",
    "# Change the corpus_foder and query_folder by the path of your data.\n",
    "corpus_folder = 'data/raw/test/image_corpus'\n",
    "query_folder = 'data/raw/test/query'\n",
    "# You can adjust the number of workers and batch size based on your system configuration.\n",
    "ref_conf_pipeline.predict(query_folder=query_folder, corpus_folder=corpus_folder, batch_size=16, num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Full pipeline (fine-tuning + infering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to modify the configuration according to your preferences.\n",
    "yml_file = 'fine_tuning/vit.yml'\n",
    "sub_config = 'torchvision'\n",
    "\n",
    "ref_conf_pipeline = RefConPipeline(yml_file, sub_config)\n",
    "\n",
    "# Change the corpus_foder and query_folder by the path of your data.\n",
    "corpus_folder = 'data/raw/test/image_corpus'\n",
    "query_folder = 'data/raw/test/query'\n",
    "# You can adjust the number of workers and batch size based on your system configuration.\n",
    "# To change workers and batch size for fine-tuning refer to the corresponding configuration file.\n",
    "ref_conf_pipeline(query_folder=query_folder, corpus_folder=corpus_folder, k=3, batch_size=16, num_workers=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reflection-connection-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
